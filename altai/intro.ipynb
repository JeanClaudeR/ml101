{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello~\n",
    "\n",
    "## Setup\n",
    "\n",
    "    git clone https://github.com/frnsys/ml101\n",
    "    cd ml101\n",
    "    pip install -r requirements.txt\n",
    "    jupyter notebook\n",
    "\n",
    "## What we'll cover today\n",
    "\n",
    "How to use:\n",
    "\n",
    "1. [`scikit-learn`](http://scikit-learn.org/stable/) for supervised linear learning\n",
    "2. [`Keras`](http://keras.io/) for neural networks\n",
    "3. [`pandas`](http://pandas.pydata.org/) for handling data\n",
    "4. [`matplotlib`](http://matplotlib.org/) and [`seaborn`](https://web.stanford.edu/~mwaskom/software/seaborn/) for visualizing data\n",
    "\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "- You have some Python experience\n",
    "- You know a bit of high school math\n",
    "\n",
    "---\n",
    "\n",
    "# Reverse Engineering Phenomena\n",
    "\n",
    "What does \"reverse engineering phenomena\" mean?\n",
    "\n",
    "Everything we see out in the world - all phenomena - whether natural or artificial, is generated by some process.\n",
    "\n",
    "This process might be extremely intricate and complex.\n",
    "\n",
    "Let's consider a house that is for sale. The price that it ultimately sells at is a result of many, many interacting components, like risky lending practices and crime in the neighborhood and the quality of schools nearby, all of which in turn have their own influences and so on.\n",
    "\n",
    "This gets messy very quickly, but we can concisely (approximately) describe this process with a function. This function can relate various relevant variables in a way that gives us the house's selling price, e.g.\n",
    "\n",
    "$$\\text{sales price} = 200 \\times \\text{square footage}$$\n",
    "\n",
    "OK, so we can describe phenomena in mathematical functions.\n",
    "\n",
    "But for a given phenomenon, how do we uncover (learn) the function that describes it?\n",
    "\n",
    "For instance, if I were given the house's selling price and a handful of other variables (it's square footage, the average level of education in the neighborhood, etc), how do I learn the function that relates the variables to the selling price? How do I _reverse engineer_ this phenomena?\n",
    "\n",
    "That's what machine learning does!\n",
    "\n",
    "_(Note that choosing these \"other variables\" is a very big part of machine learning - \"representation\" - which we'll return to)_\n",
    "\n",
    "### Some examples\n",
    "\n",
    "Determining house prices is boring, what are some more exciting examples of \"reverse engineering phenomena\"?\n",
    "\n",
    "#### AlphaGo\n",
    "\n",
    "![](../assets/alphago.png)\n",
    "\n",
    "Part of AlphaGo learns a complex function that can identify the value of a board state in Go. There's some underlying process which determines how \"good\" a board state is (perhaps based on possibly ways the opponent can play off of it) for which a heuristic function is learned.\n",
    "\n",
    "#### No Man's Sky\n",
    "\n",
    "![](../assets/nomanssky.png)\n",
    "\n",
    "This is a basically entirely procedurally generated game - all entities within the game are generated via mathematical functions.\n",
    "\n",
    "To help make things look organic and not too \"computer generated\", they use the \"[superformula](https://en.wikipedia.org/wiki/Superformula)\", which produces more natural looking shapes. You could imagine this function being reverse engineered by analyzing various shapes occuring in nature.\n",
    "\n",
    "![](../assets/superformula.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A really simple example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Plot everything as SVG\n",
    "%config InlineBackend.figure_formats=['svg']\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We'll use this later to visualize lines\n",
    "def make_line(m, b, frm=0, to=200):\n",
    "    xs = np.linspace(frm, to, 500)\n",
    "    ys = np.dot(xs[:,np.newaxis], [m]) + b\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandas can easily read CSVs into a dataframe\n",
    "df = pd.read_csv('../data/deer_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see what our data looks like\n",
    "sns.regplot(df['Weight'], df['Height'], fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data has a line-like shape.\n",
    "\n",
    "We can use _linear regression_, in which we just try to fit a line to the data we have.\n",
    "\n",
    "(Remember that a line is just a representation of a function)\n",
    "\n",
    "---\n",
    "\n",
    "## A refresher on lines\n",
    "\n",
    "A line's function:\n",
    "\n",
    "$$\n",
    "y = mx+b\n",
    "$$\n",
    "\n",
    "- $m$ is the \"slope\"\n",
    "- $b$ is the \"intercept\"\n",
    "- $y$ is the \"output\" (what we want to predict)\n",
    "- $x$ is the \"input\" (the data we want to predict from)\n",
    "\n",
    "Together, $m$ and $b$ define how a line looks.\n",
    "\n",
    "![](../assets/lines.svg)\n",
    "\n",
    "We say that $m$ and $b$ _parameterize_ the function ($m$ and $b$ are called \"parameters\").\n",
    "\n",
    "In machine learning, what we are trying to learn (reverse engineer) are these \"parameters\" because they are really what describe the function we are looking for.\n",
    "\n",
    "---\n",
    "\n",
    "In this deer height/weight example, we have:\n",
    "\n",
    "- $y$ = deer height\n",
    "- $x$ = deer weight\n",
    "- $m$ = ???\n",
    "- $b$ = ???\n",
    "\n",
    "Using known deer weights and heights (called _training data_),\n",
    "linear regression will learn $m$ and $b$ for us.\n",
    "\n",
    "\n",
    "Many of these tried-and-true machine learning algorithms, such as linear regression, are implemented and optimized in the `scikit-learn` library, which is what we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b1be33f0a944>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Note the double brackets for X -- this is necessary to get X in the right format/shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Weight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Height'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# We will use scikit-learn's LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# Note the double brackets for X -- this is necessary to get X in the right format/shape\n",
    "X = df[['Weight']]\n",
    "y = df['Height']\n",
    "\n",
    "# Create the model and \"fit\" it to the data.\n",
    "# We'll go into more detail how this works in a little bit.\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What value's for m and b did we learn?\n",
    "m = model.coef_[0]\n",
    "b = model.intercept_\n",
    "\n",
    "print('m', m)\n",
    "print('b', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our algorithm gave us m=1.8,b=50 back and thus we have learned the function:\n",
    "\n",
    "$$\n",
    "y = 1.804x + 50.095\n",
    "$$\n",
    "\n",
    "for relating a deer's weight to a deer's height.\n",
    "\n",
    "This function is called the _hypothesis_, and we can now use it to estimate deer heights given deer weights.\n",
    "\n",
    "Let's see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.regplot(df['Weight'], df['Height'], fit_reg=False)\n",
    "xs, ys = make_line(m,b)\n",
    "plt.plot(xs, ys)\n",
    "plt.legend(['$y=1.804x + 50.095$', 'data'], loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How wrong you are: Mean Squared Error\n",
    "\n",
    "One way to do so is to look at the __mean squared error__, which basically looks at how far our predicted points are off from the true points on average, penalizing greater mistakes more.\n",
    "\n",
    "For instance, if I have the data and the predicted values:\n",
    "\n",
    "| x | y true | y predicted |\n",
    "|---|--------|-------------|\n",
    "| 1 | 2      | 3           |\n",
    "| 2 | 4      | 9           |\n",
    "| 3 | 6      | 4           |\n",
    "\n",
    "The mean squared error here would be:\n",
    "\n",
    "$$\n",
    "\\frac{(3-2)^2 + (9-4)^2 + (4-6)^2}{3} = \\frac{1 + 25 + 4}{3} = 10\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "metrics.mean_squared_error(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line fits the data really well.\n",
    "\n",
    "The function I used to generate the data was:\n",
    "\n",
    "$$\n",
    "y = 1.8x + 50\n",
    "$$\n",
    "\n",
    "So our linear regression model is extremely accurate.\n",
    "\n",
    "Let's say we have a deer that weighs 150kg. We forgot to record its height. We can use our hypothesis to predict its weight:\n",
    "\n",
    "$$\n",
    "y = 1.804(150) + 50.095\n",
    "$$\n",
    "\n",
    "__Important Note__: You normally should not evaluate a model's performance on the data used to train it - if you evaluate this way, the model may _overfit_, that is, start to capture the idiosyncracies of the training data, which may not reflect the more general process you are trying to describe. The best practice is to set aside some of your data for validation and use that to evaluate your model.\n",
    "\n",
    "### Generalizing models: training & testing\n",
    "\n",
    "But there's another problem here. We are evaluating our model's performance on the data it has already seen. This gives us an overly optimistic estimate of how well we are doing because the model has already \"seen\" the correct values for each of these inputs. It won't tell us anything about how well the model __generalizes__, that is, how well it does on _new_ data.\n",
    "\n",
    "This is a common problem to watch out for in machine learning. You may have very good performance (i.e. lower error) on your training set, but do terrible elsewhere. In that case, it is likely your model is __overfit__, which is to say it fits too tightly to the pecularities of the training set.\n",
    "\n",
    "![](../assets/overfitting.png)\n",
    "(from [the Shape of Data](https://shapeofdata.files.wordpress.com))\n",
    "\n",
    "(The opposite of overfitting is underfitting, depicted on the left)\n",
    "\n",
    "The typical practice here is to divide our data into a separate __training__ and __testing__ set. That way our testing set acts as \"new\" data the model has not yet seen and you can get a better sense of how it will do on new data.\n",
    "\n",
    "There are also more sophisticated ways of getting better estimates of generalizability, but we'll stick with this for now.\n",
    "\n",
    "Let's split our data into 70% training, 30% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-14553fe5cf40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# (set random state for the purposes of demonstration)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Weight'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Height'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Weight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Height'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# (set random state for the purposes of demonstration)\n",
    "train, test = train_test_split(df[['Weight', 'Height']], train_size=0.7, random_state=1000)\n",
    "X_train = train[['Weight']]\n",
    "y_train = train['Height']\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "X_test = test[['Weight']]\n",
    "y_test = test['Height']\n",
    "y_pred = model.predict(X_test)\n",
    "metrics.mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As you can see, the error is higher now (it won't necessarily be, though). \n",
    " \n",
    " ---\n",
    "\n",
    "# How does learning happen?\n",
    "\n",
    "How does the algorithm come up with good values of $m$ and $b$?\n",
    "\n",
    "It varies, but usually via a _cost_ or _objective_ function (often notated $J$).\n",
    "\n",
    "This tells the algorithm how \"wrong\" it is with its current guesses for $m$ and $b$ on the training data.\n",
    "\n",
    "The algorithm iteratively tries different parameters (i.e. different guesses at the underlying function) until it can (approximately) minimize this error. That is, it tries to _optimize_ the parameters (thus the algorithms used for selecting these parameters are called \"optimization algorithms\").\n",
    "\n",
    "Different optimization algorithms have different ways of picking new guesses. The most popular one is _gradient descent_, which looks for the direction in which the error is decreasing, and then takes a step in that direction.\n",
    "\n",
    "If we were just finding $m$, this might look like:\n",
    "\n",
    "![](../assets/gradient_descent.svg)\n",
    "\n",
    "If we're finding both $m$ and $b$, this might look like:\n",
    "\n",
    "![](../assets/gradient_descent_3d.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_function(m):\n",
    "    y_hat = m*X.values + b\n",
    "    \n",
    "    # root mean squared error\n",
    "    return np.sqrt(np.sum((y_hat - y.values)**2)/len(X))\n",
    "\n",
    "ms = np.linspace(0,5,100)\n",
    "losses = []\n",
    "for m in ms:\n",
    "    losses.append(loss_function(m))\n",
    "plt.plot(ms, losses)\n",
    "plt.xlabel('$m$')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more complex example\n",
    "\n",
    "> The __Disposition Matrix__ is a database that United States officials describe as a \"next-generation capture/kill list\". ... The process determining criteria for killing is not public ([Wikipedia](https://en.wikipedia.org/wiki/Disposition_Matrix))\n",
    "\n",
    "> Various organizations, including the American Civil Liberties Union, have argued that Sesame Credit is more than just a meme or a game— it’s evidence that the Chinese government is enacting a scheme that will monitor citizens’ finances. More frighteningly, some have suggested, one’s political views or “morality” might raise or lower one’s score. ([Quartz](http://qz.com/519737/all-chinese-citizens-now-have-a-score-based-on-how-well-we-live-and-mine-sucks/))\n",
    "\n",
    "We could imagine in the near future, if not already, that some of this process is algorithmic. With so many signals out there - your social network activity, quantified-self data, and so on - it may be relatively trivial to automate this process at scale. The Disposition Matrix is an extreme example of the many formulas used to quantify aspects of our lives, such as our eligibility for credit or our cost for insurance and so on.\n",
    "\n",
    "Just like machine learning can uncover the function behind \"natural\" relationships like that between a deer's weight and its height, we can use it to \"reverse engineer\" human-engineered functions as well.\n",
    "\n",
    "\n",
    "## The Office of Social Health\n",
    "\n",
    "Imagine future where every citizen is assigned a \"crime coefficient\" which measures how threatening they are to society. It's sort of like big-data version of _Minority Report_'s precrime.\n",
    "\n",
    "![](../assets/social_health/safety_in_numbers.sm.jpg)\n",
    "\n",
    "We are interested in learning how these crime coefficients are computed. Most citizen's crime coefficients are public, but some peoples' are not - most notably, the police and politicians. If we can uncover the formula for computing crime coefficients, may be can we calculate these secret crime coefficients.\n",
    "\n",
    "And we have other motives too - the crime coefficient may be systematically biased, and it may not, in fact, predict crime at all.\n",
    "\n",
    "First, let's consider what data is available. As mentioned, citizens can see their own crime coefficient, and we'll assume that we have some way of collecting them.\n",
    "\n",
    "In our world there are also a variety of publicly-available datapoints.\n",
    "\n",
    "Through Facebook, we can see an individual's number of friends and their age. Through Facebook we also have access to some \"genetic\" information by seeing who their relatives are, and we can cross-reference these relatives with public arrest records to see what percent of an individual's relatives have a criminal record.\n",
    "\n",
    "We also have access to FitBit (or some other quantified-self) data, such as an individuals' average heart rate.\n",
    "\n",
    "Each of these _features_ - heart rate, criminal relation, number of Facebook friends, age, and income - may be useful in predicting crime coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>criminal_relative</th>\n",
       "      <th>friends</th>\n",
       "      <th>age</th>\n",
       "      <th>crime_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.352860</td>\n",
       "      <td>0.032889</td>\n",
       "      <td>414.0</td>\n",
       "      <td>43.039801</td>\n",
       "      <td>1.081684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49.609125</td>\n",
       "      <td>0.100910</td>\n",
       "      <td>423.0</td>\n",
       "      <td>30.572606</td>\n",
       "      <td>10.182877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.709426</td>\n",
       "      <td>0.041856</td>\n",
       "      <td>417.0</td>\n",
       "      <td>36.024089</td>\n",
       "      <td>1.751955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.092361</td>\n",
       "      <td>0.156625</td>\n",
       "      <td>408.0</td>\n",
       "      <td>55.548839</td>\n",
       "      <td>24.531254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.067936</td>\n",
       "      <td>0.158631</td>\n",
       "      <td>416.0</td>\n",
       "      <td>44.582839</td>\n",
       "      <td>25.163919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   heart_rate  criminal_relative  friends        age  crime_coef\n",
       "0   71.352860           0.032889    414.0  43.039801    1.081684\n",
       "1   49.609125           0.100910    423.0  30.572606   10.182877\n",
       "2   52.709426           0.041856    417.0  36.024089    1.751955\n",
       "3   74.092361           0.156625    408.0  55.548839   24.531254\n",
       "4   75.067936           0.158631    416.0  44.582839   25.163919"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/crimecoef_nonlinear.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.regplot(data['heart_rate'], data['crime_coef'], fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.regplot(data['criminal_relative'], data['crime_coef'], fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.regplot(data['age'], data['crime_coef'], fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.regplot(data['friends'], data['crime_coef'], fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3886268.5419214349"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try heart rate\n",
    "train, test = train_test_split(data, train_size=0.7, random_state=1000)\n",
    "X_train = train[['heart_rate']]\n",
    "y_train = train['crime_coef']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = test[['heart_rate']]\n",
    "y_test = test['crime_coef']\n",
    "y_pred = model.predict(X_test)\n",
    "metrics.mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3886803.6328770509"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try adding in friends\n",
    "train, test = train_test_split(data, train_size=0.7, random_state=1000)\n",
    "X_train = train[['heart_rate', 'friends']]\n",
    "y_train = train['crime_coef']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = test[['heart_rate', 'friends']]\n",
    "y_test = test['crime_coef']\n",
    "y_pred = model.predict(X_test)\n",
    "metrics.mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2202.059672600692"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try adding in age\n",
    "train, test = train_test_split(data, train_size=0.7, random_state=1000)\n",
    "X_train = train[['heart_rate', 'friends', 'age']]\n",
    "y_train = train['crime_coef']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = test[['heart_rate', 'friends', 'age']]\n",
    "y_test = test['crime_coef']\n",
    "y_pred = model.predict(X_test)\n",
    "metrics.mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192.89477419178218"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try adding in criminal relatives\n",
    "train, test = train_test_split(data, train_size=0.7, random_state=1000)\n",
    "X_train = train[['heart_rate', 'friends', 'age', 'criminal_relative']]\n",
    "y_train = train['crime_coef']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = test[['heart_rate', 'friends', 'age', 'criminal_relative']]\n",
    "y_test = test['crime_coef']\n",
    "y_pred = model.predict(X_test)\n",
    "metrics.mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The criminal relative feature seems to be most important. If we look at the coefficients the algorithm learned, we can see this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.72816388e-03,   6.40376599e-03,  -1.10602800e-02,\n",
       "         4.26707805e+02])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a shortcoming of machine learning - the algorithm learns to correlate criminal relatives with criminal activity but does not examine the underlying structural causes (and has no concept of it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
