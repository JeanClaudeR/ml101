{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats=['svg']\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lib import make_line\n",
    "\n",
    "palette = ['#386DF9', '#FFDC52', '#FF1614', '#62F591', '#AA22FF', '#34495E']\n",
    "sns.set(font_scale=1.1, style='darkgrid', palette=palette, context='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/classification.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot('Age', 'Time on site', data=df, fit_reg=False, hue='Purchased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we will use scikit-learn's LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# (set random state for the purposes of demonstration)\n",
    "train, test = train_test_split(df, train_size=0.7, random_state=1000)\n",
    "X_train = train[['Age', 'Time on site']]\n",
    "y_train = train['Purchased']\n",
    "\n",
    "X_test = test[['Age', 'Time on site']]\n",
    "y_test = test['Purchased']\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what value's for m and b did we learn?\n",
    "m = model.coef_[0]\n",
    "b = model.intercept_\n",
    "\n",
    "print('m', m)\n",
    "print('b', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot('Age', 'Time on site', data=df, fit_reg=False, hue='Purchased')\n",
    "\n",
    "# Solve for y = m_1x_1 + m_2x_2 + b, where y=0, so we can plot this in 2D\n",
    "def func(x):\n",
    "    return -(m[0]*x + b)/m[1]\n",
    "\n",
    "xs = np.linspace(20, 40, 500)\n",
    "ys = func(xs)\n",
    "plt.plot(xs, ys, label='Decision boundary')\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what is the model actual giving us? probabilities\n",
    "model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# although scikit-learn returns labels by default\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measuring wrongness\n",
    "\n",
    "So how did we do? With regression problems, we used the mean squared error to quantify our success.\n",
    "\n",
    "With classification, we have a different set of techniques. With classification, the two ways we can be wrong are:\n",
    "\n",
    "- Saying the point belongs to class 1 when it doesn't (__false positive__). For example, saying an email is \"spam\" when it isn't.\n",
    "- Saying the point doesn't belong to class 1 when it does (__false negative__). For example, saying an email is \"not spam\" when it is.\n",
    "\n",
    "When we guess that a point belongs to class 1 and it does, we call that a __true positive__. When a point doesn't belong to class 1 and we correctly guess that, we call that a __true negative__.\n",
    "\n",
    "We can view the true positives and negatives and the false positives and negatives together in what is called a __confusion matrix__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other common measurements include:\n",
    "\n",
    "- __recall__ - what fraction of the true class 1 items were properly identified as such?\n",
    "- __precision__ - what fraction of items identified as class 1 were truly class 1?\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/330px-Precisionrecall.svg.png) (source: [Wikipedia](https://commons.wikimedia.org/wiki/File:Precisionrecall.svg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended way, however, is to use the __area under the (ROC) curve__ (AUC).\n",
    "\n",
    "![](assets/roc.jpg) ([source](http://gim.unmc.edu/dxtests/roc3.htm))\n",
    "\n",
    "The AUC plots the false positive rate against the true positive rate for your classifier.\n",
    "\n",
    "If you make more classifier more lenient in what it classifies positively, _both_ the true positive and false positive rates will increase. A good classifier, however, will have a greater increase in true positives than in false positives.\n",
    "\n",
    "For example, in the \"excellent\" curve above, you can see that the true positive rate shoots up quickly when the false positive rate increases only a little.\n",
    "\n",
    "The \"worthless\" curve above represents an AUC score of 0.5, which is achieved if your classifier just makes random guesses.\n",
    "\n",
    "An AUC score of 1.0 is perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "metrics.roc_auc_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why not accuracy?\n",
    "\n",
    "You may be wondering, why can't we just score our results by counting the number we correctly classified over the total number of datapoints?\n",
    "\n",
    "That metric is called _accuracy_, and the reason it's not recommended is because if your dataset is _biased_ -- that is, one class is present more frequently than the other -- you might get a high accuracy simply due to chance.\n",
    "\n",
    "$$\n",
    "\\text{accuracy} = \\frac{\\text{num correct}}{\\text{num total}}\n",
    "$$\n",
    "\n",
    "For example, say I have a dataset of emails, 90% of which are labeled spam and 10% are labeled not spam.\n",
    "\n",
    "I could build a classifier which just classifies _everything_ as spam, and it would have a 0.9 accuracy (which is pretty good!). But the classifier hasn't actually _learned_ anything, and it probably won't generalize well to new email datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
